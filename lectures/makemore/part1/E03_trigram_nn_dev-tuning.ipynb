{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E03: use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xS99zWIxlYnF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "smXrGT8fkBXb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the names\n",
    "words = open('../names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, dev, and test sets\n",
    "train_index = floor(len(words) * 0.8)\n",
    "dev_index = floor(len(words) * 0.9)\n",
    "\n",
    "train = words[:train_index]\n",
    "dev = words[train_index:dev_index]\n",
    "test = words[dev_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MSYXOJdDl_WC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(['.'] + words))))\n",
    "\n",
    "# Create look up tables for the alphabet\n",
    "  # stoi = string to index\n",
    "  # itos = index to string\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Nvmdz3M6luLH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples:  182778\n"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "xs_train,  ys_train = [], []\n",
    "\n",
    "# Create the training data\n",
    "# input xs: (ch1, ch2) \n",
    "# prediction ys: ch3\n",
    "for word in train:\n",
    "\n",
    "  # prepend two special characters and append one special characters to each word\n",
    "  chs = ['.'] * 2 + list(word) + ['.']\n",
    "  # Example for 'anna\": \n",
    "  # zip(chs, chs[1:], chs[2:]) = \n",
    "  # [('.', '.', 'a'), ('.', 'a', 'n'), ('a', 'n', 'n'), ('n', 'n', 'a'), ('n', 'a', '.')]\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs_train.append((ix1, ix2))\n",
    "    ys_train.append(ix3)\n",
    "\n",
    "num = len(xs_train)\n",
    "print('number of training examples: ', num)\n",
    "xs_train = torch.tensor(xs_train)\n",
    "ys_train = torch.tensor(ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the development data\n",
    "xs_dev,  ys_dev = [], []\n",
    "for word in dev:\n",
    "  chs = ['.'] * 2 + list(word) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs_dev.append((ix1, ix2))\n",
    "    ys_dev.append(ix3)\n",
    "\n",
    "xs_dev = torch.tensor(xs_dev)\n",
    "ys_dev = torch.tensor(ys_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat the test data\n",
    "xs_test,  ys_test = [], []\n",
    "for word in test:\n",
    "  chs = ['.'] * 2 + list(word) + ['.']\n",
    "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs_test.append((ix1, ix2))\n",
    "    ys_test.append(ix3)\n",
    "\n",
    "xs_test = torch.tensor(xs_test)\n",
    "ys_test = torch.tensor(ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# weight matrix with 54 input nodes and 27 output nodes\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 0: 4.249\n",
      "loss at step 10: 2.595\n",
      "loss at step 20: 2.483\n",
      "loss at step 30: 2.429\n",
      "loss at step 40: 2.417\n",
      "loss at step 50: 2.388\n",
      "loss at step 60: 2.393\n",
      "loss at step 70: 2.371\n",
      "loss at step 80: 2.381\n",
      "loss at step 90: 2.361\n",
      "final loss: 2.377\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "iterations = 100\n",
    "learning_rate = 50\n",
    "\n",
    "for k in range(iterations):\n",
    "\n",
    "  # forward pass\n",
    "  xenc= F.one_hot(xs_train, num_classes=27).float()\n",
    "  xenc_flat = xenc.flatten(1) # flatten the one-hot encoded input vector\n",
    "  logits = xenc_flat @ W # predict log-counts\n",
    "  # softmax\n",
    "  counts = logits.exp() # counts\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  # loss function (cross-entropy) + regularization (L2)\n",
    "  loss = -probs[torch.arange(num), ys_train].log().mean() + 0.01*(W**2).mean()\n",
    "  # print loss every 10% of iterations\n",
    "  if k % floor(iterations/10) == 0:\n",
    "    print(f'loss at step {k}: {loss.item():.3f}')\n",
    "\n",
    "  # backward pass\n",
    "  W.grad = None # flush the gradients\n",
    "  loss.backward()\n",
    "\n",
    "  # update step\n",
    "  W.data += -learning_rate * W.grad\n",
    "print(f'final training loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on dev set: 2.576\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the dev set\n",
    "xenc= F.one_hot(xs_dev, num_classes=27).float()\n",
    "xenc_flat = xenc.flatten(1) # flatten the one-hot encoded input vector\n",
    "logits = xenc_flat @ W # predict log-counts\n",
    "# softmax\n",
    "counts = logits.exp() # counts\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# loss function (cross-entropy) + regularization (L2)\n",
    "loss = -probs[torch.arange(len(xs_dev)), ys_dev].log().mean() + 0.01*(W**2).mean()\n",
    "print(f'loss on dev set: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on test set: 2.593\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "xenc= F.one_hot(xs_test, num_classes=27).float()\n",
    "xenc_flat = xenc.flatten(1) # flatten the one-hot encoded input vector\n",
    "logits = xenc_flat @ W # predict log-counts\n",
    "# softmax\n",
    "counts = logits.exp() # counts\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "# loss function (cross-entropy) + regularization (L2)\n",
    "loss = -probs[torch.arange(len(xs_test)), ys_test].log().mean() + 0.01*(W**2).mean()\n",
    "print(f'loss on test set: {loss.item():.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performed worse on the dev and on the test set than on the training set.\n",
    "\n",
    "- loss on training set after $100$ iterations: $2.377$\n",
    "- loss on dev set: $2.576$\n",
    "- loss on test set: $2.593$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
