{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "random.seed(2147483647)\n",
    "random.shuffle(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(['.'] + words))))\n",
    "\n",
    "# Create look up tables for the alphabet\n",
    "  # stoi = string to index\n",
    "  # itos = index to string\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, dev, test\n",
    "train_index = int(len(words) * 0.8)\n",
    "dev_index = int(len(words) * 0.9)\n",
    "\n",
    "train = words[:train_index]\n",
    "dev = words[train_index:dev_index]\n",
    "test = words[dev_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples:  182546\n"
     ]
    }
   ],
   "source": [
    "# create the training data\n",
    "xs_train, ys_train = [], []\n",
    "for w in train:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs_train.append(ix1)\n",
    "    ys_train.append(ix2)\n",
    "xs_train = torch.tensor(xs_train)\n",
    "ys_train = torch.tensor(ys_train)\n",
    "num = xs_train.nelement()\n",
    "print('number of training examples: ', num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dev data\n",
    "xs_dev, ys_dev = [], []\n",
    "for w in dev:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs_dev.append(ix1)\n",
    "    ys_dev.append(ix2)\n",
    "xs_dev = torch.tensor(xs_dev)\n",
    "ys_dev = torch.tensor(ys_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test data\n",
    "xs_test, ys_test = [], []\n",
    "for w in test:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs_test.append(ix1)\n",
    "    ys_test.append(ix2)\n",
    "xs_test = torch.tensor(xs_test)\n",
    "ys_test = torch.tensor(ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7704033851623535\n",
      "3.3805792331695557\n",
      "3.1626715660095215\n",
      "3.028703212738037\n",
      "2.9360408782958984\n",
      "2.8688313961029053\n",
      "2.8182530403137207\n",
      "2.7787086963653564\n",
      "2.74676251411438\n",
      "2.7202842235565186\n",
      "2.6979095935821533\n",
      "2.6787352561950684\n",
      "2.6621346473693848\n",
      "2.6476519107818604\n",
      "2.634939670562744\n",
      "2.6237220764160156\n",
      "2.613773822784424\n",
      "2.6049106121063232\n",
      "2.5969772338867188\n",
      "2.58984375\n",
      "2.5834014415740967\n",
      "2.577559232711792\n",
      "2.572240114212036\n",
      "2.567378282546997\n",
      "2.5629193782806396\n",
      "2.5588159561157227\n",
      "2.555028200149536\n",
      "2.551522731781006\n",
      "2.5482699871063232\n",
      "2.5452449321746826\n",
      "2.542426586151123\n",
      "2.539795398712158\n",
      "2.537335157394409\n",
      "2.5350310802459717\n",
      "2.532870292663574\n",
      "2.53084135055542\n",
      "2.528932571411133\n",
      "2.5271358489990234\n",
      "2.525442123413086\n",
      "2.523843288421631\n",
      "2.5223331451416016\n",
      "2.520904541015625\n",
      "2.5195517539978027\n",
      "2.5182690620422363\n",
      "2.517052412033081\n",
      "2.5158965587615967\n",
      "2.5147976875305176\n",
      "2.5137522220611572\n",
      "2.512755870819092\n",
      "2.511805772781372\n",
      "2.510899305343628\n",
      "2.5100340843200684\n",
      "2.509207010269165\n",
      "2.508415460586548\n",
      "2.507657766342163\n",
      "2.506931781768799\n",
      "2.5062360763549805\n",
      "2.505568265914917\n",
      "2.504927635192871\n",
      "2.5043118000030518\n",
      "2.5037198066711426\n",
      "2.503150463104248\n",
      "2.5026028156280518\n",
      "2.502074956893921\n",
      "2.501566171646118\n",
      "2.5010762214660645\n",
      "2.500603199005127\n",
      "2.5001466274261475\n",
      "2.4997060298919678\n",
      "2.4992799758911133\n",
      "2.498868703842163\n",
      "2.4984707832336426\n",
      "2.4980859756469727\n",
      "2.497713088989258\n",
      "2.4973528385162354\n",
      "2.4970035552978516\n",
      "2.4966650009155273\n",
      "2.496337413787842\n",
      "2.4960193634033203\n",
      "2.495710849761963\n",
      "2.4954121112823486\n",
      "2.495121717453003\n",
      "2.494840145111084\n",
      "2.4945666790008545\n",
      "2.494300603866577\n",
      "2.494042158126831\n",
      "2.493791103363037\n",
      "2.4935474395751953\n",
      "2.4933104515075684\n",
      "2.493079662322998\n",
      "2.4928555488586426\n",
      "2.4926371574401855\n",
      "2.492424726486206\n",
      "2.492217779159546\n",
      "2.4920167922973633\n",
      "2.4918205738067627\n",
      "2.4916293621063232\n",
      "2.491443634033203\n",
      "2.491262197494507\n",
      "2.4910855293273926\n",
      "final training loss: 2.491\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs_train, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys_train].log().mean() + 0.01*(W**2).mean()\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad\n",
    "print(f'final training loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on dev set: 2.488\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the dev set\n",
    "xenc_dev = F.one_hot(xs_dev, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc_dev @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(len(xs_dev)), ys_dev].log().mean() + 0.01*(W**2).mean()\n",
    "print(f\"loss on dev set: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on test set: 2.486\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "xenc_test = F.one_hot(xs_test, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc_test @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(len(xs_test)), ys_test].log().mean() + 0.01*(W**2).mean()\n",
    "print(f\"loss on test set: {loss.item():.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performed better on the dev and on the test set than on the training set.\n",
    "\n",
    "- loss on training set after $100$ iterations: $2.491$\n",
    "- loss on dev set: $2.488$\n",
    "- loss on test set: $2.486$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
