{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# makemore: part 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weights are all 0, the inputs to the activation function will be 0. However, evaluating tanh at 0 yields 0, so if we perform backpropagation, and calculate the gradient, we get 0. This means that the weights will not be updated, and the network will not learn anything. Only the bias of the final layer can actually learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('../names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182437, 3]) torch.Size([182437])\n",
      "torch.Size([22781, 3]) torch.Size([22781])\n",
      "torch.Size([22928, 3]) torch.Size([22928])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes we create here are the same API as nn.Module in PyTorch\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters(epoch, layers):\n",
    "  print(f'epoch {epoch}')\n",
    "  if epoch == 0:\n",
    "    for i, layer in enumerate(layers):\n",
    "      if isinstance(layer, Linear):\n",
    "        print(f'layer {i} ({layer.__class__.__name__})')\n",
    "        print(f'  weight: {layer.weight[0][0]}')\n",
    "        print(f'  bias: {layer.bias[0]}')\n",
    "      if isinstance(layer, BatchNorm1d):\n",
    "        print(f'layer {i} ({layer.__class__.__name__})')\n",
    "        print(f'  gamma: {layer.gamma[0]}')\n",
    "        print(f'  beta: {layer.beta[0]}')\n",
    "      if isinstance(layer, Tanh):\n",
    "        print(f'layer {i} ({layer.__class__.__name__})')\n",
    "    print('\\n')\n",
    "  else:\n",
    "    for i, layer in enumerate(layers):\n",
    "      if isinstance(layer, Linear):\n",
    "        print(f'layer {i} ({layer.__class__.__name__})')\n",
    "        print(f'  weight: {layer.weight[0][0]}')\n",
    "        print(f'  weight_grad: {layer.weight.grad[0][0]}')\n",
    "        print(f'  bias: {layer.bias[0]}')\n",
    "        print(f'  bias_grad: {layer.bias.grad[0]}')\n",
    "        print(f' output: {layer.out[0][0]}')\n",
    "        print(f' output_grad: {layer.out.grad[0][0]}')\n",
    "      if isinstance(layer, BatchNorm1d):\n",
    "        print(f'layer {i} ({layer.__class__.__name__})')\n",
    "        print(f'  gamma: {layer.gamma[0]}')\n",
    "        print(f'  gamma_grad: {layer.gamma.grad[0]}')\n",
    "        print(f'  beta: {layer.beta[0]}')\n",
    "        print(f'  beta_grad: {layer.beta.grad[0]}')\n",
    "      if isinstance(layer, Tanh):\n",
    "        print(f'layer {i} ({layer.__class__.__name__})')\n",
    "        print(f' output: {layer.out[0][0]}')\n",
    "        print(f' output_grad: {layer.out.grad[0][0]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13470\n"
     ]
    }
   ],
   "source": [
    "# Let's train a deeper network\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# layers = [\n",
    "#   Linear(n_embd * block_size, n_hidden, bias=True), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=True), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=True), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=True), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden, bias=True), BatchNorm1d(n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size, bias=True), BatchNorm1d(vocab_size),\n",
    "# ]\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden), \n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 0.0 #1.0 #5/3\n",
    "      layer.bias *= 0.0\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "layer 0 (Linear)\n",
      "  weight: -0.0\n",
      "  bias: 0.0\n",
      "layer 1 (Tanh)\n",
      "layer 2 (Linear)\n",
      "  weight: 0.0\n",
      "  bias: 0.0\n",
      "\n",
      "\n",
      "epoch 1\n",
      "layer 0 (Linear)\n",
      "  weight: -0.0\n",
      "  weight_grad: 0.0\n",
      "  bias: 0.0\n",
      "  bias_grad: 0.0\n",
      " output: 0.0\n",
      " output_grad: 0.0\n",
      "layer 1 (Tanh)\n",
      " output: 0.0\n",
      " output_grad: 0.0\n",
      "layer 2 (Linear)\n",
      "  weight: 0.0\n",
      "  weight_grad: 0.0\n",
      "  bias: 0.024000002071261406\n",
      "  bias_grad: -0.24000000953674316\n",
      " output: 0.0\n",
      " output_grad: 0.0003124999930150807\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "\n",
    "  print_parameters(i, layers)\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for layer in layers:\n",
    "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 \n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "  \n",
    "  if i >= 1:\n",
    "    break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see how the gradients are squished by the tanh layer in comparison with the 0 weight gradient from the linear layer. Only the bias of the last linear layer can learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
